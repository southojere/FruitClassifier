# -*- coding: utf-8 -*-
"""transferLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14eY3MrM_4NHgBDQgefeN7t8dsLAxpLcn
"""

import numpy as np  
from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  
from keras.models import Sequential  
from keras.layers import Dropout, Flatten, Dense  
from keras import applications
import matplotlib.pyplot as plt  
import math  
import cv2
from keras.utils.np_utils import to_categorical  

from skimage import data, exposure, img_as_float

from google.colab import drive
drive.mount('/content/drive')

# dimensions of our images.
img_width, img_height = 300, 300  

top_model_weights_path = 'drive/My Drive/UNI/2018/COMP309/Project Image classifier/complete_model.h5'
train_data_dir = 'drive/My Drive/UNI/2018/COMP309/Project Image classifier/Train_data'
validation_data_dir = 'drive/My Drive/UNI/2018/COMP309/Project Image classifier/Validation_data'
epochs = 50
batch_size = 32
!pip freeze | grep tensorflow

def save_bottlebeck_features():
  """
    In this function we create the VGG16 model, without the fully connected layers,
    by specifying include_top=False and load using ImageNet weights
  """
  model = applications.VGG16(include_top=False, weights='imagenet')
  datagen = ImageDataGenerator(rescale=1. / 255)
  
  generator = datagen.flow_from_directory(  
     train_data_dir,  
     target_size=(img_width, img_height),  
     batch_size=batch_size,  
     class_mode=None,  
     shuffle=False)
  
  nb_train_samples = len(generator.filenames);

  num_classes = len(generator.class_indices)
  
  predict_size_train = int(math.ceil(nb_train_samples / batch_size)) #for fixing a bug, we cal it ourselfs
  bottleneck_features_train = model.predict_generator(  
     generator, predict_size_train)
  np.save('drive/My Drive/UNI/2018/COMP309/Project Image classifier/bottleneck_features_train.npy', bottleneck_features_train)
  
  #Validation datas turn
  generator = datagen.flow_from_directory(  
     validation_data_dir,  
     target_size=(img_width, img_height),  
     batch_size=batch_size,  
     class_mode=None,  
     shuffle=False)  
   
  nb_validation_samples = len(generator.filenames)  
   
  predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))  
   
  bottleneck_features_validation = model.predict_generator(  
     generator, predict_size_validation)  
   
  np.save('drive/My Drive/UNI/2018/COMP309/Project Image classifier/bottleneck_features_validation.npy', bottleneck_features_validation)

save_bottlebeck_features()

def train_top_model():
  '''
    Now its time to actually train the top layers of the model aka. the last layers
    We have our bottleneck extracted features using our pre trained model, now we use those to 
    finish off our classification.
    
    
    
  '''
# 1) in order to train the top model, we need the class labels for each training/val instance
  datagen_top = ImageDataGenerator('''preprocessing_function = exposure.equalize_hist''')
  generator_top = datagen_top.flow_from_directory(  
         train_data_dir,  
         target_size=(img_width, img_height),  
         batch_size=batch_size,  
         class_mode='categorical',  
         shuffle=False)  
   
  nb_train_samples = len(generator_top.filenames)  
  num_classes = len(generator_top.class_indices)
  
  # load the bottleneck features saved earlier  
  train_data = np.load('drive/My Drive/UNI/2018/COMP309/Project Image classifier/bottleneck_features_train.npy')
  
  # get the class labels for the training data, in the original order  
  train_labels = generator_top.classes 
   
  # convert the training labels to categorical vectors  
  train_labels = to_categorical(train_labels, num_classes=num_classes)
  
  '''Repeat for validation set'''
  
  generator_top = datagen_top.flow_from_directory(  
         validation_data_dir,  
         target_size=(img_width, img_height),  
         batch_size=batch_size,  
         class_mode=None,  
         shuffle=False)  
   
  nb_validation_samples = len(generator_top.filenames)  
   
  validation_data = np.load('drive/My Drive/UNI/2018/COMP309/Project Image classifier/bottleneck_features_validation.npy')  
   
  validation_labels = generator_top.classes
  validation_labels = to_categorical(validation_labels, num_classes=num_classes)
  
  '''Now we train a small fully connected network as our top model, 
     using the bottle neck features as inputs'''
  
  model = Sequential()  
  model.add(Flatten(input_shape=train_data.shape[1:]))
  print('hERE: ',train_data.shape[1:])
  model.add(Dense(256, activation='relu'))  
  model.add(Dropout(0.5))  
  model.add(Dense(num_classes, activation='softmax')) 
  
  model.compile(optimizer='Adam',  
              loss='categorical_crossentropy', metrics=['accuracy'])
  
  history = model.fit(train_data, train_labels,  
          epochs=epochs,  
          batch_size=batch_size,  
          validation_data=(validation_data, validation_labels))
  
  model.save(top_model_weights_path)#saving our model
  
  (eval_loss, eval_accuracy) = model.evaluate(  
     validation_data, validation_labels, batch_size=batch_size, verbose=1)
  
  print("[INFO] accuracy: {:.2f}%".format(eval_accuracy * 100))  
  print("[INFO] Loss: {}".format(eval_loss))
  
  '''Plotting training history'''
  
  plt.figure(1)  
   
  # summarize history for accuracy  
   
  plt.subplot(211)  
  plt.plot(history.history['acc'])  
  plt.plot(history.history['val_acc'])  
  plt.title('model accuracy')  
  plt.ylabel('accuracy')  
  plt.xlabel('epoch')  
  plt.legend(['train', 'test'], loc='upper left')  
   
  # summarize history for loss  
   
  plt.subplot(212)  
  plt.plot(history.history['loss'])  
  plt.plot(history.history['val_loss'])  
  plt.title('model loss')  
  plt.ylabel('loss')  
  plt.xlabel('epoch')  
  plt.legend(['train', 'test'], loc='upper left')  
  plt.show()

!pip install keras==2.2.4

train_top_model()

!pip install tensorflow==1.11.0

!pip freeze | grep tensorflow

